{
  "hash": "b89e7da68db137f7d1aac745e04383d5",
  "result": {
    "markdown": "---\ntitle: \"PhDs in Progress\"\ndate: 2021-12-01\ncategories:\n  - \"Issue 2\"\nimage: \"photo_ACaron.jpg\" \nlinks: \n   - title: PhDs in Progress\n     description: \"Projects currently being undertaken by the department’s PhD students… \" \n     image: photo_ACaron.jpg\n---\n\n\n> *Projects currently being undertaken by the department’s PhD students…*\n\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](photo_ACaron.jpg){fig-align='center' fig-alt='Headshot of Alberto Caron' width=45%}\n:::\n:::\n\n\n# Alberto Caron\n\n*Supervisors: Ioanna Manolopoulou and Gianluca Baio*\n\nMy PhD research project mainly revolves around using Bayesian non-parametric (or Bayesian machine learning) regression methods to carry out causal/counterfactual inference under high-dimensional observational data. \n\nCounterfactual learning with observational data is of interest in many disciplines, such as healthcare and socio-economic sciences, where the exploration of alternative actions or policies in the real-world, through randomization, is costly, unethical or practically unfeasible, and where highly personalised decision-making is core (e.g. a medical treatment). Bayesian ML methods, if duly adjusted to the causal setting, provide useful and flexible tools for counterfactual prediction and uncertainty quantification around the impact of an action on an outcome of interest, given a certain (high-dimensional) context/covariate space. \n\nOur contributions so far have extended some popular models in the field. Our first work extends the popular Bayesian causal forests learner, to induce more shrinkage/regularisation (hence the name Shrinkage Bayesian Causal Forests) to make it adapt better to sparse data generating processes and discover the main drivers behind the heterogeneity in the response e.g. to a certain treatment. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](photo_ACaron-page-001.jpg){fig-align='center' width=90%}\n:::\n:::\n\n\nOur second work develops a multi-output deep kernel learning model to deal with causal settings where both context and action space are large, and where one is interested in the impact on multiple correlated outcomes. Current work focuses instead on importance sampling techniques and mediation analysis.\n\n\n\n:::\n\n::: {.column width=\"5%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](photo_MMamajiwala.jpg){fig-align='center' fig-alt='Headshot of Mariya Mamajiwala' width=45%}\n:::\n:::\n\n\n# Mariya Mamajiwala \n\n*Supervisor: Serge Guillas*\n\nMy research interests lie at the intersection of Monte Carlo methods, differential geometry and stochastics. The aim is to use concepts from differential geometry to develop efficient Monte Carlo algorithms.\n\nOne of the algorithms I have developed using Langevin dynamics on Riemannian manifolds is for the optimisation of non-convex problems. There are two important aspects to this method. The first is to write a given Ito stochastic differential equation on a Riemannian manifold which may be looked upon as the generalisation of the famous Laplace-Beltrami operator. The second is to define the Riemannian manifold on which to constrain the Langevin diffusion; towards this we have shown how to derive inspiration from some aspects of energetics to define a Riemannian metric which thus defines the manifold. Based on these two aspects we propose the differential geometric counterpart of a Euclidean optimisation method based on Langevin dynamics. The comparison between the Euclidean and geometric approaches is shown in the figure for a 40-dimensional Ackley function (true solution is the 40-dimensional zero vector).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](photo_MMamajiwala.png){width=90%}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](photo_MMamajiwala (2).png){width=1440}\n:::\n:::\n\n\nA similar approach is also used to develop algorithms in Markov chain Monte Carlo, machine learning and Bayesian calibration. Moreover, this has also been applied to parameter estimation within the framework of a combined state and parameter estimation problem considered in data assimilation wherein – unlike the other problems mentioned earlier – data is available as a time history.\n\n\n\n\n:::\n\n::::\n\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](photo_XLiang.jpg){fig-align='center' fig-alt='Headshot of Xitong Liang' width=45%}\n:::\n:::\n\n\n# Xitong Liang\n\n*Supervisors: Jim Griffin and Samuel Livingstone*\n\nIn the modern high-dimensional setting, a variable selection method for a particular statistical model can not only help make better predictions but can also be used to investigate underlying low-dimensional structure based on the belief that only a small subset of predictors play a crucial role in influencing the response.\n\nA variable selection method is an automatic procedure that selects the best subset of regressors that explains most of the variation in the response among a massive number of regressors. Bayesian variable selection methods using spike-and-slab priors can be used to extract natural measures of uncertainty such as posterior model probabilities and marginal posterior variable inclusion probabilities. It has been shown that spike-and-slab priors often lead to posterior consistency in the sense that the posterior distribution puts more and more mass around the true model as more observations are gathered. \n\nThe exact posterior distribution when using a Bayesian spike-and-slab prior is challenging to compute. Markov chain Monte Carlo (MCMC) algorithms are typically used to estimate posterior summaries of interest when the number of regressors is larger than 30. \n\nMy primary PhD research objective is developing efficient MCMC algorithms targeting discrete-valued high-dimensional distributions, such as posterior distributions in Bayesian variable selection problems. \n\nWe introduce a new framework, Random Neighbourhood Samplers, and show that many recently introduced algorithms can be viewed as particular cases within the framework. We also describe a novel algorithm, the Adaptive Random Neighbourhood Informed sampler, by combining ideas from some of these existing approaches and show, using several examples of both real and simulated datasets, that a computationally efficient point-wise implementation leads to reliable inferences on a range of variable selection problems, particularly in the very large $p$ setting. \n\n\n:::\n\n::: {.column width=\"5%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](photo_KLi.jpg){fig-align='center' fig-alt='Headshot of Kaiyu Li' width=45%}\n:::\n:::\n\n\n# Kaiyu Li\n\n*Supervisor: Serge Guillas*\n\nI am interested in uncertainty quantification and machine learning, with a particular focus on Gaussian processes and their application to tsunamis in Indonesia.\n\nThe simulation of a future Indonesian tsunami will be implemented by using a partial differential equation solver called VOLNA. It is a tsunami simulator that simulates the life cycle of tsunami waves. For the purpose of uncertainty quantification, many tsunami scenarios need to be generated by running tsunami simulators on high performance computing facilities. However, due to the high computational cost of tsunami simulators, it is impractical to do this. Gaussian processes are widely used as surrogate models for spatio-temporal data emulation and prediction, and their properties make them suitable for emulating these kinds of expensive tsunami models. The idea is that we approximate the key features of the expensive computer model using the cheaper Gaussian process surrogate, allowing us to test various scenarios more quickly without having to run the full model as many times as we would otherwise.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](photo_XLiang.png){fig-align='center' width=90%}\n:::\n:::\n\n\nI want to design novel learning algorithms to improve the efficiency of emulation, prediction and uncertainty propagation, from the aspects of computational linear algebra methods and multi-fidelity modelling. By using computational linear algebraic methods, Gram matrices of Gaussian processes can have a specific structure, e.g. low rank or sparse. The cubic computational cost of using a Gaussian process can also be reduced. The use of multi-fidelity models will also reduce the time cost of data simulation. These algorithms can be applied for assessing the risk of future tsunamis in Indonesia.\n\n\n\n:::\n\n::::\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](photo_ZSun.png){fig-align='center' fig-alt='Hadshot of Zhuo Sun' width=45%}\n:::\n:::\n\n\n# Zhuo Sun\n\n*Supervisor: François-Xavier Briol*\n\nI am working on transfer learning in computational statistics and machine learning. In particular, I have been working on control variates, a post-processing tool to reduce the variance of Markov chain Monte Carlo (MCMC) estimators.\n\nIntegrals appear in many fields of statistics and machine learning. For instance, the expectation of some integrand against some target distribution is often of interest in statistics – posterior means of parameters, model evidence and other quantities of interest can all be written as integrals. The most common tools for these expectations are MCMC estimators. Control variates are constructed on the basis of Stein operators and the Stein class of the distribution of interest, which only includes functions with zero-mean. By subtracting the control variates from the integrand and taking the average, we get control variates estimators for those integrals, which are known to have much lower variance than that of standard MCMC estimators.\n\nA recent work of mine is called Vector-valued Control Variates, which generalises the idea of control variates from scalar-valued to vector-valued integrals. We achieve this by proposing novel matrix-valued Stein reproducing kernels. Estimating multiple related integrals jointly can be beneficial as we would expect the information to be shared among these integration tasks. Our experiments also show that vector-valued control variates outperform scalar-valued control variates in many scenarios.\n\n\n:::\n\n::: {.column width=\"5%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"45%\"}\n\n<!--another empty column-->\n\n:::\n\n::::\n\n\n**Stay on top of the latest news and research coming from the Department of Statistical Science by visiting: [www.ucl.ac.uk/statistics](https://www.ucl.ac.uk/statistics).**",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}